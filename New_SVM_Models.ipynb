{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1140 assets, index the returned LazyList to import.\n"
     ]
    }
   ],
   "source": [
    "import menpo.io as mio\n",
    "import os\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import Models\n",
    "from Models import ChangeVector, CenterPointLandmark\n",
    "path_to_svm_training_database = '/Programing/GR/Code/CK+/aam-images/**/**/**/*'\n",
    "\n",
    "\n",
    "def process(image, crop_proportion=0.2, max_diagonal=400):\n",
    "    if image.n_channels == 3:\n",
    "        image = image.as_greyscale()\n",
    "    image = image.crop_to_landmarks_proportion(crop_proportion)\n",
    "    d = image.diagonal()\n",
    "    if d > max_diagonal:\n",
    "        image = image.rescale(float(max_diagonal) / d)\n",
    "    return image\n",
    "\n",
    "#process changingVector, reduce dimension from 2x68 to 1x68, by process PCA\n",
    "def pca(changeVector):\n",
    "    X = np.array(changeVector)\n",
    "    pca_model = PCA(n_components=1)\n",
    "    return pca_model.fit_transform(X)\n",
    "\n",
    "\n",
    "training_images = mio.import_images(path_to_svm_training_database, verbose=True)\n",
    "training_images = training_images.map(process)\n",
    "\n",
    "path_to_facs = '/Programing/GR/Code/CK+/FACS/'\n",
    "path_to_emotions = '/Programing/GR/Code/CK+/Emotion/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def landmark_normalize(landmark):\n",
    "    vector_max = 0\n",
    "    for p in landmark:\n",
    "        if(math.sqrt(float(p[0])*float(p[0]) + float(p[1])*float(p[1])) > vector_max):\n",
    "            vector_max = math.sqrt(p[0]*p[0] + p[1]*p[1])\n",
    "    for p in landmark:\n",
    "        p[0] = p[0]/float(vector_max)\n",
    "        p[1] = p[1]/float(vector_max)\n",
    "    return landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_center_point_landmark(landmark):\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "    xmean = 0\n",
    "    ymeam = 0\n",
    "    for p in landmark:\n",
    "        x_array.append(p[0])\n",
    "        y_array.append(p[1])\n",
    "#     return x_array, y_array\n",
    "    xmean = np.mean(x_array)\n",
    "    ymean = np.mean(y_array)\n",
    "    center_point_landmark = []\n",
    "    for i in range(0,68):\n",
    "        center_point_landmark.append([x_array[i] - float(xmean), y_array[i] - float(ymean)])\n",
    "    return landmark_normalize(center_point_landmark)\n",
    "#     return x_array, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create training data\n",
    "from Models import CenterPointLandmark\n",
    "\n",
    "count = 0;\n",
    "svm_training_data = []\n",
    "while(count < len(training_images)):\n",
    "    file_path = str(training_images[count].path).split(\"\\\\\")\n",
    "    facs_path = path_to_facs + file_path[6] + '/' + file_path[7]\n",
    "    gt_emotion = file_path[7]\n",
    "    facs_path = facs_path + '/' + os.listdir(facs_path)[0]\n",
    "    fi = open(facs_path, 'r')\n",
    "    data_facs = []\n",
    "    for line in fi: # read rest of lines\n",
    "        for x in line.split():\n",
    "            if(int(float(x)) not in data_facs and int(float(x))!= 0):\n",
    "                data_facs.append(int(float(x)))\n",
    "    data_facs.sort()\n",
    "    fi.close()\n",
    "    \n",
    "    centerPointLandmark = []\n",
    "#     landmark_neutral = process_center_point_landmark(training_images[count].landmarks['PTS'].lms.points)\n",
    "    landmark_perk = process_center_point_landmark(training_images[count + 1].landmarks['PTS'].lms.points)\n",
    "\n",
    "#     svm_training_data.append(CenterPointLandmark([], landmark_neutral, gt_emotion))\n",
    "    svm_training_data.append(CenterPointLandmark(data_facs, landmark_perk, gt_emotion))\n",
    "    count = count + 2\n",
    "#     centerPointVector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 20, 21, 25]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_training_data[1].facs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "facs = []\n",
    "#create facs array\n",
    "for data in svm_training_data:\n",
    "    for facs_code in data.facs:\n",
    "        if(int(facs_code) not in facs and int(facs_code)!= 0):\n",
    "            facs.append(facs_code)\n",
    "facs.sort()\n",
    "\n",
    "#create model for each action unit in facs[]\n",
    "models = []\n",
    "au_models_score = []\n",
    "# SVC\n",
    "# for au in facs:\n",
    "#     x_training = []\n",
    "#     y_label = []\n",
    "#     #create label array\n",
    "#     for data in svm_training_data:\n",
    "#         if(au in data.facs):\n",
    "#             y_label.append(1)\n",
    "#         else:\n",
    "#             y_label.append(0)\n",
    "#         #create training data: 1x68 array, result of PCA process\n",
    "#         vector = []\n",
    "#         for tmp in data.landmarkChange:\n",
    "#             vector.append(tmp[0])\n",
    "#             vector.append(tmp[1])\n",
    "#         x_training.append(vector)\n",
    "#     x_training_normalized = normalize(x_training, norm='max')\n",
    "#     clf = svm.LinearSVC()\n",
    "#     clf.fit(x_training_normalized, y_label)\n",
    "#     au_models_score.append(clf.score(x_training, y_label))\n",
    "#     models.append(clf)\n",
    "\n",
    "#Random forest\n",
    "x_training = []\n",
    "y_label = []\n",
    "#create label array\n",
    "for data in svm_training_data:\n",
    "#     y_tmp.append(data.facs)\n",
    "    #create training data: 1x68 array, result of PCA process\n",
    "    vector = []\n",
    "#     y_tmp = [0 for x in range(max(facs) +1)]\n",
    "#     if(len(data.facs) > 0):\n",
    "#         for f in data.facs:\n",
    "#             y_tmp[f] = 1\n",
    "    y_tmp = [0 for x in range(20)]\n",
    "    y_tmp[int(data.emotion_label)] = 1\n",
    "    y_label.append(y_tmp)\n",
    "        \n",
    "    for tmp in data.landmarkChange:\n",
    "        vector.append(tmp[0])\n",
    "        vector.append(tmp[1])\n",
    "    x_training.append(vector)\n",
    "x_training_normalized = normalize(x_training, norm='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'010'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.emotion_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4801817407945572]\n",
      "Found 46 assets, index the returned LazyList to import.\n",
      "#######\n",
      "Score: \n",
      "0.5797143259217562\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(100, random_state = 0)\n",
    "# clf = svm.LinearSVC()\n",
    "model = MultiOutputRegressor(clf)\n",
    "model.fit(x_training_normalized, y_label)\n",
    "au_models_score.append(model.score(x_training, y_label))\n",
    "\n",
    "print(au_models_score)\n",
    "\n",
    "#create testing data\n",
    "svm_testing_data = []\n",
    "path_to_svm_testing_database = \"/Programing/GR/Code/CK+/test-aam-images/**/**/**/*\"\n",
    "testing_images = mio.import_images(path_to_svm_testing_database, verbose=True)\n",
    "testing_images = testing_images.map(process)\n",
    "\n",
    "count = 0;\n",
    "while(count < len(testing_images)):\n",
    "    file_path = str(testing_images[count].path).split(\"\\\\\")\n",
    "    facs_path = path_to_facs + file_path[6] + '/' + file_path[7]\n",
    "    gt_emotion = file_path[7]\n",
    "    facs_path = facs_path + '/' + os.listdir(facs_path)[0]\n",
    "    fi = open(facs_path, 'r')\n",
    "    data_facs = []\n",
    "    for line in fi: # read rest of lines\n",
    "        for x in line.split():\n",
    "            if(int(float(x)) not in data_facs and int(float(x)) != 0):\n",
    "                data_facs.append(int(float(x)))\n",
    "    #print(array)\n",
    "    fi.close()\n",
    "    \n",
    "    centerPointLandmark = []\n",
    "#     landmark_neutral = process_center_point_landmark(testing_images[count].landmarks['PTS'].lms.points)\n",
    "    landmark_perk = process_center_point_landmark(testing_images[count + 1].landmarks['PTS'].lms.points)\n",
    "\n",
    "#     svm_testing_data.append(CenterPointLandmark([], landmark_neutral, gt_emotion))\n",
    "    svm_testing_data.append(CenterPointLandmark(data_facs, landmark_perk, gt_emotion))\n",
    "    count = count + 2\n",
    "\n",
    "    \n",
    "#evaluate trained model with test data and get score\n",
    "print('#######')\n",
    "print('Score: ')\n",
    "# SVC score\n",
    "# for au in facs:\n",
    "#     x_training = []\n",
    "#     y_label = []\n",
    "#     #create label array\n",
    "#     for data in svm_testing_data:\n",
    "#         if(au in data.facs):\n",
    "#             y_label.append(1)\n",
    "#         else:\n",
    "#             y_label.append(0)\n",
    "#         #create training data: 1x68 array, result of PCA process\n",
    "#         vector = []\n",
    "#         for tmp in data.landmarkChange:\n",
    "#             vector.append(tmp[0])\n",
    "#             vector.append(tmp[1])\n",
    "#         x_training.append(vector)\n",
    "#     print(models[facs.index(au)].score(x_training, y_label))\n",
    "    \n",
    "# Random Forest score\n",
    "x_training = []\n",
    "y_label = []\n",
    "#create label array\n",
    "for data in svm_testing_data:\n",
    "#     y_tmp = [0 for x in range(max(facs) +1)]\n",
    "#     if(len(data.facs) > 0):\n",
    "#         for f in data.facs:\n",
    "#             y_tmp[f] = 1\n",
    "    y_tmp = [0 for x in range(20)]\n",
    "    y_tmp[int(data.emotion_label)] = 1\n",
    "    y_label.append(y_tmp)\n",
    "    #create training data: 1x68 array, result of PCA process\n",
    "    vector = []\n",
    "    for tmp in data.landmarkChange:\n",
    "        vector.append(tmp[0])\n",
    "        vector.append(tmp[1])\n",
    "    x_training.append(vector)\n",
    "print(model.score(x_training, y_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5399615 , -0.90301431, -0.29876248, -0.90150064, -0.0620954 ,\n",
       "        -0.87703413,  0.17019436, -0.81914416,  0.39685127, -0.74220763,\n",
       "         0.60260231, -0.62224664,  0.7706943 , -0.45134974,  0.90658033,\n",
       "        -0.25673187,  0.92967959, -0.02457927,  0.9035998 ,  0.23118287,\n",
       "         0.80127929,  0.46811442,  0.63515176,  0.66385767,  0.4302579 ,\n",
       "         0.82048945,  0.19294654,  0.91614754, -0.05986159,  0.97130905,\n",
       "        -0.31559602,  0.99975543, -0.57212124,  1.        , -0.58666985,\n",
       "        -0.78926123, -0.70134957, -0.64635451, -0.73031159, -0.4644722 ,\n",
       "        -0.70005212, -0.28071379, -0.65013761, -0.10309227, -0.68757359,\n",
       "         0.10857452, -0.70004397,  0.29216899, -0.75147976,  0.4692407 ,\n",
       "        -0.73847537,  0.65086323, -0.63949463,  0.80382369, -0.51710168,\n",
       "        -0.00463808, -0.35501027, -0.01708243, -0.19402578, -0.02822695,\n",
       "        -0.03330257, -0.03210684,  0.13736285, -0.26085433,  0.14806096,\n",
       "        -0.1399006 ,  0.15106891, -0.02032761,  0.14801039,  0.11680805,\n",
       "         0.14216621,  0.25384553, -0.44073599, -0.66262666, -0.51937052,\n",
       "        -0.5114956 , -0.52074327, -0.34387918, -0.41540914, -0.21641617,\n",
       "        -0.3925503 , -0.37189022, -0.37424473, -0.52606527, -0.3989508 ,\n",
       "         0.22837392, -0.53421805,  0.34263329, -0.54457053,  0.5231924 ,\n",
       "        -0.46134514,  0.68234793, -0.404609  ,  0.53788664, -0.3994741 ,\n",
       "         0.38323308,  0.41513915, -0.39115524,  0.33877166, -0.27452591,\n",
       "         0.26310212, -0.15673956,  0.27766696, -0.02399381,  0.27444084,\n",
       "         0.12316879,  0.35402568,  0.24741681,  0.45903663,  0.35349592,\n",
       "         0.45819154,  0.22486863,  0.48940415,  0.09978801,  0.49273832,\n",
       "        -0.02839857,  0.48094476, -0.1527304 ,  0.43223769, -0.26919067,\n",
       "         0.37466702, -0.30192483,  0.37421425, -0.21269435,  0.38008495,\n",
       "        -0.03275676,  0.38340095,  0.16958047,  0.38393552,  0.26153823,\n",
       "         0.38447001,  0.16618966,  0.38154931, -0.03275683,  0.37511986,\n",
       "        -0.21581565]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize([x_training[1]], norm='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####\n",
      "[[1, 1], [2, 1], [15, 1], [17, 1]]\n",
      "4\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 0]\n",
      "[15, 1]\n",
      "[17, 1]\n",
      "[23, 0]\n",
      "#####\n",
      "[[2, 1], [4, 1], [17, 1]]\n",
      "2\n",
      "---\n",
      "[1, 0]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[15, 0]\n",
      "[17, 1]\n",
      "[23, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [25, 1]]\n",
      "3\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 0]\n",
      "[5, 0]\n",
      "[14, 0]\n",
      "[17, 0]\n",
      "[20, 0]\n",
      "[25, 1]\n",
      "[26, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [15, 1], [17, 1]]\n",
      "5\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[15, 1]\n",
      "[17, 1]\n",
      "#####\n",
      "[[2, 1], [4, 1], [17, 1]]\n",
      "3\n",
      "---\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[17, 1]\n",
      "[23, 0]\n",
      "[38, 0]\n",
      "#####\n",
      "[[2, 1], [4, 1]]\n",
      "0\n",
      "---\n",
      "[14, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [25, 1]]\n",
      "4\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[14, 0]\n",
      "[16, 0]\n",
      "[20, 0]\n",
      "[25, 1]\n",
      "[38, 0]\n",
      "#####\n",
      "[[4, 1], [7, 1], [17, 1], [23, 1], [24, 1]]\n",
      "3\n",
      "---\n",
      "[2, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[17, 1]\n",
      "[23, 1]\n",
      "[38, 0]\n",
      "#####\n",
      "[[4, 1], [17, 1]]\n",
      "0\n",
      "---\n",
      "[14, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [15, 1], [17, 1]]\n",
      "4\n",
      "---\n",
      "[1, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[15, 1]\n",
      "[17, 1]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [17, 1]]\n",
      "3\n",
      "---\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[9, 0]\n",
      "[15, 0]\n",
      "[17, 1]\n",
      "[23, 0]\n",
      "[30, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1]]\n",
      "0\n",
      "---\n",
      "[14, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [25, 1]]\n",
      "3\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 0]\n",
      "[5, 0]\n",
      "[14, 0]\n",
      "[17, 0]\n",
      "[20, 0]\n",
      "[25, 1]\n",
      "[26, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [15, 1]]\n",
      "4\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[4, 1]\n",
      "[15, 1]\n",
      "[17, 0]\n",
      "#####\n",
      "[[17, 1]]\n",
      "0\n",
      "---\n",
      "[14, 0]\n",
      "#####\n",
      "[[1, 1], [4, 1], [15, 1], [17, 1]]\n",
      "4\n",
      "---\n",
      "[1, 1]\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[15, 1]\n",
      "[17, 1]\n",
      "[38, 0]\n",
      "#####\n",
      "[[4, 1], [7, 1], [17, 1], [23, 1], [24, 1]]\n",
      "4\n",
      "---\n",
      "[2, 0]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[6, 0]\n",
      "[7, 1]\n",
      "[17, 1]\n",
      "[23, 1]\n",
      "#####\n",
      "[[4, 1], [17, 1]]\n",
      "0\n",
      "---\n",
      "[14, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [20, 1]]\n",
      "4\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[20, 1]\n",
      "[25, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [15, 1], [17, 1]]\n",
      "5\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[15, 1]\n",
      "[17, 1]\n",
      "[26, 0]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [17, 1]]\n",
      "0\n",
      "---\n",
      "[14, 0]\n",
      "#####\n",
      "[[2, 1], [4, 1], [17, 1], [23, 1], [24, 1]]\n",
      "3\n",
      "---\n",
      "[1, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[17, 1]\n",
      "[23, 1]\n",
      "#####\n",
      "[[1, 1], [2, 1], [4, 1], [20, 1], [25, 1]]\n",
      "5\n",
      "---\n",
      "[1, 1]\n",
      "[2, 1]\n",
      "[3, 0]\n",
      "[4, 1]\n",
      "[5, 0]\n",
      "[10, 0]\n",
      "[14, 0]\n",
      "[16, 0]\n",
      "[20, 1]\n",
      "[25, 1]\n",
      "90\n",
      "0.3917184265010351\n"
     ]
    }
   ],
   "source": [
    "wrong_predict = 0\n",
    "au_score = []\n",
    "for data in svm_testing_data:\n",
    "    print('#####')\n",
    "    local_wrong_predict = 0\n",
    "    local_accurate_predict = 0\n",
    "    tmp = []\n",
    "    predict = []\n",
    "    \n",
    "    for vector in data.landmarkChange:\n",
    "        tmp.append(vector[0])\n",
    "        tmp.append(vector[1])\n",
    "        \n",
    "    for model in models:\n",
    "        if(model.predict([tmp]) >= 0.5):\n",
    "            predict.append([facs[models.index(model)], model.predict([tmp])[0]])\n",
    "            #print(facs[models.index(model)])\n",
    "            if(facs[models.index(model)] not in data.facs):\n",
    "                local_wrong_predict += 1\n",
    "            else: \n",
    "                local_accurate_predict += 1\n",
    "        else:\n",
    "            if(facs[models.index(model)] in data.facs):\n",
    "                local_wrong_predict += 1\n",
    "    print(predict)\n",
    "    print(local_accurate_predict)\n",
    "    if(len(data.facs) != 0):\n",
    "        au_score.append(float(local_accurate_predict)/float(len(data.facs)))\n",
    "    print(\"---\")\n",
    "    data.facs.sort()\n",
    "    for gt_facs in data.facs:\n",
    "        print([gt_facs, models[facs.index(gt_facs)].predict([tmp])[0]])\n",
    "    wrong_predict += local_wrong_predict\n",
    "    \n",
    "print(wrong_predict)\n",
    "print(sum(au_score)/float(len(au_score)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:GR]",
   "language": "python",
   "name": "conda-env-GR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
